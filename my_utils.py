import io
import numpy as np
from PIL import Image
import torch
import torch.nn.functional as F

from dataset.intrinsic.pipeline import run_pipeline

from dep_nor.utils.projection import intrins_from_fov
import torchvision.transforms as transforms
import dep_nor.utils.utils as utils

## Utils
# å°‡æ¨¡å‹(æˆ–æ¨¡å‹å­—å…¸)æ¬åˆ°æŒ‡å®š device
def _move_to_device(obj, device):
    if hasattr(obj, "to"):
        obj.to(device)
    elif isinstance(obj, dict):
        for v in obj.values():
            _move_to_device(v, device)
    elif isinstance(obj, (list, tuple)):
        for v in obj:
            _move_to_device(v, device)

def alb_to_pil(tensor):
	# ---- è™•ç†æˆ PIL ----
	# å»æ‰ batch ç¶­åº¦
	if tensor.ndim == 4:
		tensor = tensor[0]

	# æŠŠå€¼åŸŸå¾ [-1,1] æˆ– [0,1] è½‰æˆ [0,255]
	if tensor.min() < 0:
		img = (tensor.clamp(-1, 1) + 1) / 2
	else:
		img = tensor.clamp(0, 1)

	# è½‰æˆ CPU numpy
	img = img.detach().cpu().permute(1, 2, 0).numpy()  # [H,W,C]
	img = (img * 255).astype("uint8")

	# è½‰æˆ PIL Image
	pil_img = Image.fromarray(img)
	return pil_img

def img_transforms(img, mode='input_cond'):
    if mode == 'input_cond':
        image_transforms = transforms.Compose([
			transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.BILINEAR),
			transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]
		])
    elif mode == 'side_cond':
        image_transforms = transforms.Compose([
            transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.BILINEAR),
        ])
        
    if type(img) != torch.Tensor:
        img = transforms.ToTensor()(img)
        
    return image_transforms(img)



## Albedo Estimation
def pil_from_any(obj):
    if isinstance(obj, Image.Image):
        return obj
    if isinstance(obj, dict) and "bytes" in obj:
        return Image.open(io.BytesIO(obj["bytes"]))
    if isinstance(obj, (bytes, bytearray)):
        return Image.open(io.BytesIO(obj))
    raise TypeError(f"Unsupported image type: {type(obj)}")

def to_rgb_numpy(img):
    """
    å°‡è¼¸å…¥çµ±ä¸€æˆ np.uint8 çš„ HxWx3 é™£åˆ—ã€‚
    æ”¯æ´ PIL.Image èˆ‡ numpy.ndarrayã€‚
    """
    if isinstance(img, Image.Image):
        img = img.convert("RGB")
        arr = np.array(img)  # HxWx3, uint8
    elif isinstance(img, np.ndarray):
        arr = img
        if arr.ndim == 2:  # ç°éš -> ç–Šä¸‰é€šé“
            arr = np.stack([arr]*3, axis=-1)
        elif arr.ndim == 3 and arr.shape[2] == 4:  # RGBA -> ä¸Ÿ alpha
            arr = arr[:, :, :3]
        elif arr.ndim != 3 or arr.shape[2] != 3:
            raise ValueError(f"Unsupported ndarray shape: {arr.shape}")
        if arr.dtype != np.uint8:
            arr = arr.astype(np.uint8)
    else:
        raise TypeError(f"Unsupported image type: {type(img)}")
    return arr

def to_pil_rgb(arr):
    """æŠŠè¼¸å…¥çµ±ä¸€è½‰æˆ PIL RGBã€‚æ”¯æ´ numpy/PILï¼›è‡ªå‹•è™•ç†ç°éšã€RGBAã€float(0~1/0~255)ã€‚"""
    if isinstance(arr, Image.Image):
        return arr.convert("RGB")

    # ä¸æ˜¯ PILï¼Œå°±ç•¶æˆ numpy
    if not isinstance(arr, np.ndarray):
        arr = np.array(arr)

    # ç°éš -> ç–Š 3 é€šé“
    if arr.ndim == 2:
        arr = np.stack([arr]*3, axis=-1)

    # RGBA -> ä¸Ÿ alpha
    if arr.ndim == 3 and arr.shape[-1] == 4:
        arr = arr[:, :, :3]

    # æµ®é» â†’ uint8
    if np.issubdtype(arr.dtype, np.floating):
        # å¦‚æœæœ€å¤§å€¼ <= 1 è¦–ç‚º 0~1ï¼Œæ”¾å¤§åˆ° 0~255
        if arr.max() <= 1.0:
            arr = (arr * 255.0).round()
        arr = np.clip(arr, 0, 255).astype(np.uint8)
    elif arr.dtype != np.uint8:
        arr = arr.astype(np.uint8)

    return Image.fromarray(arr, mode="RGB")

def gen_albedo(ex, intrinsic_model, device):
	pil_img = pil_from_any(ex)
	np_rgb = to_rgb_numpy(pil_img)

	# é—œéµã€Œå°æ”¹ã€ï¼šè½‰æˆ float32 ä¸¦ç¸®æ”¾åˆ° 0~1ï¼Œé¿å…ä¸‹æ¸¸è®Šæˆ float64
	np_rgb = np_rgb.astype(np.float32) / 255.0

	_move_to_device(intrinsic_model, device)

	result = run_pipeline(
		intrinsic_model,
		np_rgb,   # å‚³ numpy float32ï¼Œtorch.from_numpy æœƒç¶­æŒ float32
		device=device
	)

	_move_to_device(intrinsic_model, 'cpu')
	torch.cuda.empty_cache()

	def view(img, p=100):
		img = img ** (1/2.2)
		return (img / np.percentile(img, p)).clip(0, 1)

	alb = view(result["hr_alb"])
	alb_tensor = img_transforms(torch.from_numpy(alb).permute(2, 0, 1).contiguous().unsqueeze(0))
	return alb_tensor


## DSINE: depth & normal estimation
def depth_estimation(img, pipe, device):
    pipe.model.to(device)
    pipe.device = torch.device(device) if not isinstance(device, torch.device) else device
    depth = pipe(img)["depth"]

    pipe.model.to('cpu')
    pipe.device = torch.device('cpu')
    torch.cuda.empty_cache()

    return depth

def normal_estimation(img, model, device):
    _move_to_device(model, device)
    img = img.convert("RGB")
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

    img = np.array(img).astype(np.float32) / 255.0
    img = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).to(device)
    _, _, orig_H, orig_W = img.shape
    lrtb = utils.get_padding(orig_H, orig_W)
    img = F.pad(img, lrtb, mode="constant", value=0.0)
    img = normalize(img)

    intrins = intrins_from_fov(new_fov=60.0, H=orig_H, W=orig_W, device=device).unsqueeze(0)
    intrins[:, 0, 2] += lrtb[0]
    intrins[:, 1, 2] += lrtb[2]

    pred_norm = model(img, intrins=intrins)[-1]
    _move_to_device(model, 'cpu')
    torch.cuda.empty_cache()

    pred_norm = pred_norm[:, :, lrtb[2]:lrtb[2]+orig_H, lrtb[0]:lrtb[0]+orig_W]

    pred_norm = pred_norm.detach().cpu().permute(0, 2, 3, 1).numpy()
    pred_norm = (((pred_norm + 1) * 0.5) * 255).astype(np.uint8)
    im = Image.fromarray(pred_norm[0,...])

    return im

def normal_estimation_sn(img, predictor, device):
    _move_to_device(predictor, device)
    img = img.convert("RGB")
    normal = predictor(img)
    _move_to_device(predictor, 'cpu')
    return normal


## Lightmap Estimation
def _to_0_1(x: torch.Tensor) -> torch.Tensor:
    """
    å°‡å½±åƒ tensor å¤§è‡´è½‰åˆ° [0,1]ã€‚
    - è‹¥çœ‹èµ·ä¾†æ˜¯ [-1,1]ï¼Œå°± (x+1)/2
    - è‹¥æœ¬ä¾†å°±åœ¨ [0,1] å·¦å³ï¼Œå°±ä¸å‹•
    """
    x_min = float(x.min())
    x_max = float(x.max())
    if x_min < -0.1 or x_max > 1.1:   # ç²—ç•¥åˆ¤æ–·æ˜¯ [-1,1]
        x = (x.clamp(-1.0, 1.0) + 1.0) / 2.0
    return x


def calculate_pred_lightmap(
    I_relit: torch.Tensor,
    pseudo_gt: torch.Tensor,
) -> torch.Tensor:
    """
    Args:
        I_relit:   Bx3xHxWï¼ŒVAE decode å‡ºä¾†çš„ relit åœ–
        pseudo_gt: Bx3xHxWï¼Œpseudo GTï¼ˆä¾‹å¦‚ã€Œç‡ˆé—œæ‰ã€æˆ–åŸåœ–ï¼‰
    Returns:
        lightmap_pred: Bx1xHxWï¼Œç°éšï¼Œå€¼ç´„åœ¨ [0,1]
        ğŸ‘‰ åªä»£è¡¨ã€Œé€™ç›ç‡ˆåœ¨è©²å¼·åº¦ä¸‹è®“å“ªè£¡è®Šäº®å¤šå°‘ã€ï¼Œä¸å«é¡è‰²
    """
    device = I_relit.device
    dtype = I_relit.dtype

    I_r = _to_0_1(I_relit)
    I_g = _to_0_1(pseudo_gt)

    # RGB -> luminanceï¼šY = 0.299R + 0.587G + 0.114B
    w = torch.tensor([0.299, 0.587, 0.114], device=device, dtype=dtype).view(1, 3, 1, 1)
    L_r = (I_r * w).sum(dim=1, keepdim=True)  # Bx1xHxW
    L_g = (I_g * w).sum(dim=1, keepdim=True)  # Bx1xHxW

    # åªä¿ç•™ã€Œè®Šäº®ã€çš„éƒ¨åˆ†ï¼Œè®Šæš—å°±ç•¶ä½œåˆ¥çš„å…‰æºè¢«å£“æ‰ã€å…ˆä¸ç®¡
    dL = torch.clamp(L_r - L_g, min=0.0)      # Bx1xHxW

    # å¯é¸ï¼šclip å¤§ outlierï¼Œé¿å… loss è¢«å¹¾å€‹å¥‡æ€ªçš„é»ä¸»å°
    dL = dL.clamp(0.0, 1.0)

    return dL  # Bx1xHxW